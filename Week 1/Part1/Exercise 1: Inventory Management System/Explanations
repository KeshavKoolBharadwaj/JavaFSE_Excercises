Q1) Why Data Structures and Algorithms are Essential in Handling Large Inventories:
    •	Efficiency: Proper data structures and algorithms ensure efficient data storage, retrieval, and management, which is crucial for handling large inventories.
    •	Scalability: As the inventory grows, the system must handle increased data without performance degradation.
    •	Speed: Quick access to inventory data (e.g., searching for a product, updating quantities) is critical for operational efficiency.
  •	Memory Management: Efficient data structures help in optimal memory usage, preventing wastage and ensuring the system can handle large datasets.


Q2) Types of Data Structures Suitable for This Problem:
    •	HashMap: Ideal for scenarios where quick search, insert, and delete operations are required, as it allows for average O(1) time complexity for these operations.


Q3) Implementation of the problem statement:
    • Check the product.java and inventory.java files

Q4) Analyze the time complexity of each operation (add, update, delete) in your chosen data structure.
    • Data Structure Chosen: HashMap<String, Product>

    • Time complexities of each methods:
        1) Add Product (addProduct):
              Operation: Insert a product into the HashMap.
              
              Time Complexity:
                  Average Case: O(1) - HashMap uses a hashing mechanism, which on average, allows for constant-time insertion.
                  Worst Case: O(n) - This can occur if many hash collisions happen, and all elements are placed in the same bucket, effectively making the bucket a
                                     linked list.
        
        2) Update Product (updateProduct):
              
              Operation: Update the product details if the product exists in the HashMap.
             
              Time Complexity:
                  Average Case: O(1) - Finding and updating an entry in the HashMap is on average constant time.
                  Worst Case: O(n) - Similar to insertion, in the worst case, due to hash collisions, finding an element could take linear time.
   
        3) Delete Product (deleteProduct):
    
              Operation: Remove a product from the HashMap.
              
              Time Complexity:
                  Average Case: O(1) - Removing an entry by its key is constant time on average.
                  Worst Case: O(n) - Again, due to possible hash collisions, it could degrade to linear time in the worst case.
          
    • Discuss how you can optimize these operations.
        
        To optimize the operations further, several strategies can be implemented:
        
        1) Choosing an Appropriate Initial Capacity and Load Factor:
              Initial Capacity: Setting an initial capacity that is close to the expected number of entries can reduce the number of rehash operations.
              Load Factor: The load factor (default is 0.75) controls the trade-off between space and time efficiency. A lower load factor reduces the chance of
                            collision but increases space usage.
        2) Using Better Hash Functions:
              Custom Hash Function: Ensuring the hash function distributes keys uniformly can reduce the likelihood of collisions, improving average-case performance.
        
        3) Handling Collisions Efficiently:
              Separate Chaining with Linked List: The default in Java’s HashMap is separate chaining using linked lists, which can be improved by using other 
                                                  data structures.
              Separate Chaining with Balanced Trees: Starting from Java 8, HashMap switches from linked lists to balanced trees (e.g., red-black trees) when a 
                                                     bucket exceeds a certain threshold. This ensures that even in cases of hash collisions, the time complexity 
                                                     remains O(log n).
        
        4) Concurrent Access:
              ConcurrentHashMap: For multi-threaded environments, using ConcurrentHashMap instead of HashMap ensures thread-safe operations without significant 
                                 performance hits.
        
        5) Rehashing Strategy:
              Efficient Rehashing: Rehashing is an expensive operation, so using a rehashing strategy that minimizes its frequency can improve performance. 
                                   For instance, doubling the array size during rehashing can reduce the number of times rehashing is needed.
